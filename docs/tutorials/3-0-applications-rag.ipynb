{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Structured Output to RAG applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is RAG?**\n",
    "\n",
    "Retrieval Augmented Generation (RAG) models are the bridge between large language models and external knowledge databases. They fetch the relevant data for a given query. For example, if you have some documents and want to ask questions related to the content of those documents, RAG models help by retrieving data from those documents and passing it to the LLM in queries.\n",
    "\n",
    "**How do RAG models work?**\n",
    "\n",
    "The typical RAG process involves embedding a user query and searching a vector database to find the most relevant information to supplement the generated response. This approach is particularly effective when the database contains information closely matching the query but not more than that.\n",
    "\n",
    "![Image](https://jxnl.github.io/instructor/blog/img/dumb_rag.png)\n",
    "\n",
    "**Why is there a need for them?**\n",
    "\n",
    "Pre-trained large language models do not learn over time. If you ask them a question they have not been trained on, they will often hallucinate. Therefore, we need to embed our own data to achieve a better output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RAG\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n",
    "\n",
    "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
    "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
    "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the RAG model\n",
    "\n",
    "**What's the solution?**\n",
    "\n",
    "Enhancing RAG requires a more sophisticated approach known as query understanding.\n",
    "\n",
    "This process involves analyzing the user's query and transforming it to better match the backend's search capabilities.\n",
    "\n",
    "By doing so, we can significantly improve both the precision and recall of the search results, providing more accurate and relevant responses.\n",
    "\n",
    "![Image](https://jxnl.github.io/instructor/blog/img/query_understanding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples below, we're going to use the [`instructor`](https://github.com/jxnl/instructor) library to simplify the interaction between the programmer and language models via the function-calling API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq instructor anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "\n",
    "from anthropic import AnthropicBedrock\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "client = instructor.from_anthropic(\n",
    "    AnthropicBedrock(), \n",
    "    mode=instructor.Mode.ANTHROPIC_JSON,\n",
    "    max_tokens=1024,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1) Improving Extractions\n",
    "\n",
    "One of the big limitations is that often times the query we embed and the text\n",
    "we are searching for may not have a direct match, leading to suboptimal results.\n",
    "A common method of using structured output is to extract information from a\n",
    "document and use it to answer a question. Directly, we can be creative in how we\n",
    "extract, summarize and generate potential questions in order for our embeddings\n",
    "to do better.\n",
    "\n",
    "For example, instead of using just a text chunk we could try to:\n",
    "\n",
    "1. extract key words and themes\n",
    "2. extract hypothetical questions\n",
    "3. generate a summary of the text\n",
    "\n",
    "In the example below, we use the `instructor` library to extract the key words\n",
    "and themes from a text chunk and use them to answer a question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extraction(BaseModel):\n",
    "    topic: str\n",
    "    summary: str\n",
    "    hypothetical_questions: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Hypothetical questions that this document could answer\",\n",
    "    )\n",
    "    keywords: List[str] = Field(\n",
    "        default_factory=list, description=\"Keywords that this document is about\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hypothetical_questions': ['How can I improve the performance of my RAG '\n",
      "                            'system?',\n",
      "                            'What are the key limitations of a simple RAG '\n",
      "                            'implementation?',\n",
      "                            'How can I make my RAG system more flexible and '\n",
      "                            'adaptable to different data sources?'],\n",
      " 'keywords': ['RAG',\n",
      "              'Vector Database',\n",
      "              'Query-Document Mismatch',\n",
      "              'Monolithic Search Backend',\n",
      "              'Text Search Limitations',\n",
      "              'Limited Planning Ability'],\n",
      " 'summary': 'The simplest implementation of RAG embeds a user query and does a '\n",
      "            'single embedding search in a vector database, like a vector store '\n",
      "            'of Wikipedia articles. However, this approach often falls short '\n",
      "            'when dealing with complex queries and diverse data sources.',\n",
      " 'topic': 'Simple RAG'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "text_chunk = \"\"\"\n",
    "## Simple RAG\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n",
    "\n",
    "**What are the limitations?**\n",
    "\n",
    "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
    "    - Query: \"Tell me about climate change effects on marine life.\"\n",
    "    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n",
    "- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n",
    "    - Query: \"Latest research in quantum computing.\"\n",
    "    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n",
    "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
    "    - Query: \"what problems did we fix last week\"\n",
    "    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n",
    "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n",
    "    - Query: \"Tips for first-time Europe travelers.\"\n",
    "    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n",
    "\"\"\"\n",
    "\n",
    "extractions = client.chat.completions.create(\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    max_tokens=1024,\n",
    "    stream=True,\n",
    "    response_model=Iterable[Extraction],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Your role is to extract chunks from the following and create a set of topics.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "for extraction in extractions:\n",
    "    pprint(extraction.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can imagine if you were to embed the summaries, hypothetical questions,\n",
    "and keywords in a vector database (i.e. in the metadata fields of a vector\n",
    "database), you can then use a vector search to find the best matching document\n",
    "for a given query. What you'll find is that the results are much better than if\n",
    "you were to just embed the text chunk!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2) Understanding 'recent queries' to add temporal context\n",
    "\n",
    "One common application of using structured outputs for query understanding is to identify the intent of a user's query. In this example we're going to use a simple schema to seperately process the query to add additional temporal context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str\n",
    "    published_daterange: DateRange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `DateRange` and `Query` are Pydantic models that structure the user's query with a date range and a list of domains to search within.\n",
    "\n",
    "These models **restructure** the user's query by including a <u>rewritten query</u>, a <u>range of published dates</u>, and a <u>list of domains</u> to search in.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new restructured query, we can apply this pattern to our function calls to obtain results that are optimized for our backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query(rewritten_query='What happened in June 2024?', published_daterange=DateRange(start=datetime.date(2024, 6, 1), end=datetime.date(2024, 6, 30)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_query(q) -> Query:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "query = expand_query(\"What happened last month\")\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't just about adding some date ranges. We can even use some chain of thought prompting to generate tailored searches that are deeply integrated with our backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query(rewritten_query='recent developments in artificial intelligence', published_daterange=DateRange(chain_of_thought='To find the most relevant and up-to-date information on recent developments in AI, I should search for information published within the last 1-2 years.', start=datetime.date(2022, 7, 16), end=datetime.date(2024, 7, 16)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DateRange(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str = Field(\n",
    "        description=\"Rewrite the query to make it more specific\"\n",
    "    )\n",
    "    published_daterange: DateRange = Field(\n",
    "        description=\"Effective date range to search in\"\n",
    "    )\n",
    "\n",
    "\n",
    "def expand_query(q) -> Query:\n",
    "    return client.chat.completions.create(\n",
    "        model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "        max_tokens=1024,\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "expand_query(\"What are some recent developments in AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Weights and Biases to track experiments\n",
    "\n",
    "While running a function like this production is quite simple, a lot of time will be spend on iterating and improving the model. To do this, we can use Weights and Biases to track our experiments.\n",
    "\n",
    "In order to do so we wand manage a few things\n",
    "\n",
    "1. Save input and output pairs for later\n",
    "2. Save the JSON schema for the response_model\n",
    "3. Having snapshots of the model and data allow us to compare results over time, and as we make changes to the model we can see how the results change.\n",
    "\n",
    "This is particularly useful when we might want to blend a mix of synthetic and real data to evaluate our model. We can use the `wandb` library to track our experiments and save the results to a dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU wandb pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import instructor\n",
    "import logging\n",
    "\n",
    "from anthropic import AsyncAnthropicBedrock\n",
    "from datetime import date\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class DateRange(BaseModel):\n",
    "    chain_of_thought: str = Field(\n",
    "        description=\"Think step by step to plan what is the best time range to search in\"\n",
    "    )\n",
    "    start: date\n",
    "    end: date\n",
    "\n",
    "\n",
    "class Query(BaseModel):\n",
    "    rewritten_query: str = Field(\n",
    "        description=\"Rewrite the query to make it more specific\"\n",
    "    )\n",
    "    published_daterange: DateRange = Field(\n",
    "        description=\"Effective date range to search in\"\n",
    "    )\n",
    "\n",
    "    def report(self):\n",
    "        dct = self.model_dump()\n",
    "        dct[\"usage\"] = self._raw_response.usage.model_dump()\n",
    "        return dct\n",
    "\n",
    "\n",
    "\n",
    "# We'll use a different client for async calls\n",
    "# To highlight the difference and how we can use both\n",
    "aclient = instructor.from_anthropic(AsyncAnthropicBedrock(), mode=instructor.Mode.ANTHROPIC_JSON)\n",
    "\n",
    "\n",
    "async def expand_query(\n",
    "    q, *, model: str = \"anthropic.claude-3-sonnet-20240229-v1:0\", temp: float = 0\n",
    ") -> Query:\n",
    "    return await aclient.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temp,\n",
    "        max_tokens=1024,\n",
    "        response_model=Query,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"You're a query understanding system for the Metafor Systems search engine. Today is {date.today()}. Here are some tips: ...\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": f\"query: {q}\"},\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % pip install pandas wandb\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def flatten_dict(d: Dict[str, Any], parent_key: str = \"\", sep: str = \"_\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary.\n",
    "\n",
    "    :param d: The nested dictionary to flatten.\n",
    "    :param parent_key: The base key to use for the flattened keys.\n",
    "    :param sep: Separator to use between keys.\n",
    "    :return: A flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def dicts_to_df(list_of_dicts: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert a list of dictionaries to a pandas DataFrame.\n",
    "\n",
    "    :param list_of_dicts: List of dictionaries, potentially nested.\n",
    "    :return: A pandas DataFrame representing the flattened data.\n",
    "    \"\"\"\n",
    "    # Flatten each dictionary and create a DataFrame\n",
    "    flattened_data = [flatten_dict(d) for d in list_of_dicts]\n",
    "    return pd.DataFrame(flattened_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstephenhibbert\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/stephibb/prototype/instructor/docs/tutorials/wandb/run-20240716_092656-ghqbbfow</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stephenhibbert/query/runs/ghqbbfow' target=\"_blank\">legendary-voice-11</a></strong> to <a href='https://wandb.ai/stephenhibbert/query' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stephenhibbert/query' target=\"_blank\">https://wandb.ai/stephenhibbert/query</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stephenhibbert/query/runs/ghqbbfow' target=\"_blank\">https://wandb.ai/stephenhibbert/query/runs/ghqbbfow</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 8.0%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average duration (s)</td><td>▁</td></tr><tr><td>duration (s)</td><td>▁</td></tr><tr><td>n_queries</td><td>▁</td></tr><tr><td>usage_input_tokens</td><td>▁</td></tr><tr><td>usage_output_tokens</td><td>▁</td></tr><tr><td>usage_total_tokens</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average duration (s)</td><td>1.1293</td></tr><tr><td>duration (s)</td><td>4.51721</td></tr><tr><td>n_queries</td><td>4</td></tr><tr><td>usage_input_tokens</td><td>1741</td></tr><tr><td>usage_output_tokens</td><td>499</td></tr><tr><td>usage_total_tokens</td><td>2240</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">legendary-voice-11</strong> at: <a href='https://wandb.ai/stephenhibbert/query/runs/ghqbbfow' target=\"_blank\">https://wandb.ai/stephenhibbert/query/runs/ghqbbfow</a><br/> View project at: <a href='https://wandb.ai/stephenhibbert/query' target=\"_blank\">https://wandb.ai/stephenhibbert/query</a><br/>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240716_092656-ghqbbfow/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "model=\"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "temp = 0\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"query\",\n",
    "    config={\"model\": model, \"temp\": temp},\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"latest developments in artificial intelligence last 3 weeks\",\n",
    "    \"renewable energy trends past month\",\n",
    "    \"quantum computing advancements last 2 months\",\n",
    "    \"biotechnology updates last 10 days\",\n",
    "]\n",
    "\n",
    "start = time.perf_counter()\n",
    "queries = await asyncio.gather(\n",
    "    *[expand_query(q, model=model, temp=temp) for q in test_queries]\n",
    ")\n",
    "duration = time.perf_counter() - start\n",
    "\n",
    "with open(\"schema.json\", \"w+\") as f:\n",
    "    schema = Query.model_json_schema()\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "with open(\"results.jsonlines\", \"w+\") as f:\n",
    "    for query in queries:\n",
    "        f.write(query.model_dump_json() + \"\\n\")\n",
    "\n",
    "df = dicts_to_df([q.report() for q in queries])\n",
    "df[\"input\"] = test_queries\n",
    "df.to_csv(\"results.csv\")\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"usage_total_tokens\": df[\"usage_output_tokens\"].sum() + df[\"usage_input_tokens\"].sum(),\n",
    "        \"usage_output_tokens\": df[\"usage_output_tokens\"].sum(),\n",
    "        \"usage_input_tokens\": df[\"usage_input_tokens\"].sum(),\n",
    "        \"duration (s)\": duration,\n",
    "        \"average duration (s)\": duration / len(queries),\n",
    "        \"n_queries\": len(queries),\n",
    "    }\n",
    ")\n",
    "\n",
    "run.log(\n",
    "    {\n",
    "        \"results\": wandb.Table(dataframe=df),\n",
    "    }\n",
    ")\n",
    "\n",
    "files = wandb.Artifact(\"data\", type=\"dataset\")\n",
    "files.add_file(\"schema.json\")\n",
    "files.add_file(\"results.jsonlines\")\n",
    "files.add_file(\"results.csv\")\n",
    "\n",
    "run.log_artifact(files)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of Weights and Biases would return something like the below table.\n",
    "\n",
    "| Metric                   | Value  |\n",
    "|--------------------------|--------|\n",
    "| average duration (s)     | 1.5945 |\n",
    "| duration (s)             | 6.37799|\n",
    "| n_queries                | 4      |\n",
    "| usage_completion_tokens  | 376    |\n",
    "| usage_prompt_tokens      | 780    |\n",
    "| usage_total_tokens       | 1156   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3) Personal Assistants, parallel processing\n",
    "\n",
    "A personal assistant application needs to interpret vague queries and fetch information from multiple backends, such as emails and calendars. By modeling the assistant's capabilities using Pydantic, we can dispatch the query to the correct backend and retrieve a unified response.\n",
    "\n",
    "For instance, when you ask, \"What's on my schedule today?\", the application needs to fetch data from various sources like events, emails, and reminders. This data is stored across different backends, but the goal is to provide a consolidated summary of results.\n",
    "\n",
    "It's important to note that the data from these sources may not be embedded in a search backend. Instead, they could be accessed through different clients like a calendar or email, spanning both personal and professional accounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "class SearchClient(BaseModel):\n",
    "    query: str = Field(description=\"The search query that will go into the search bar\")\n",
    "    keywords: List[str]\n",
    "    email: str\n",
    "    source: Literal[\"gmail\", \"calendar\"]\n",
    "    date_range: DateRange\n",
    "\n",
    "\n",
    "class Retrieval(BaseModel):\n",
    "    queries: List[SearchClient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can utilize this with a straightforward query such as \"What do I have today?\".\n",
    "\n",
    "The system will attempt to asynchronously dispatch the query to the appropriate backend.\n",
    "\n",
    "However, it's still crucial to remember that effectively prompting the language model is still a key aspect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"emails and calendar events for today related to work\",\n",
      "            \"keywords\": [\n",
      "                \"work\",\n",
      "                \"meeting\",\n",
      "                \"agenda\",\n",
      "                \"deadline\",\n",
      "                \"project\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"Since the request is for today's work items, the best date range to search is just today's date, 2024-07-16.\",\n",
      "                \"start\": \"2024-07-16\",\n",
      "                \"end\": \"2024-07-16\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"calendar events scheduled for today\",\n",
      "            \"keywords\": [\n",
      "                \"meeting\",\n",
      "                \"appointment\",\n",
      "                \"event\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"calendar\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"To find calendar events for today, I will search Jason's work calendar for the date range of just 2024-07-16.\",\n",
      "                \"start\": \"2024-07-16\",\n",
      "                \"end\": \"2024-07-16\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrieval = client.chat.completions.create(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    response_model=Retrieval,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
    "                He has two emails jason@work.com jason@personal.com\n",
    "                Today is {date.today()}\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"What do I have today for work? any new emails?\"},\n",
    "    ],\n",
    ")\n",
    "print(retrieval.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more challenging, we will assign it multiple tasks, followed by a list of queries that are routed to various search backends, such as email and calendar. Not only do we dispatch to different backends, over which we have no control, but we are also likely to render them to the user in different ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"queries\": [\n",
      "        {\n",
      "            \"query\": \"Meetings for today\",\n",
      "            \"keywords\": [\n",
      "                \"meeting\",\n",
      "                \"schedule\",\n",
      "                \"calendar\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"calendar\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"To find meetings for today, I should search Jason's work calendar for events on the current date of 2024-07-12.\",\n",
      "                \"start\": \"2024-07-12\",\n",
      "                \"end\": \"2024-07-12\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"query\": \"Important emails from the last week\",\n",
      "            \"keywords\": [\n",
      "                \"important\",\n",
      "                \"high priority\",\n",
      "                \"urgent\"\n",
      "            ],\n",
      "            \"email\": \"jason@work.com\",\n",
      "            \"source\": \"gmail\",\n",
      "            \"date_range\": {\n",
      "                \"chain_of_thought\": \"To find important emails from the last week, I should search Jason's work email for emails marked as important or high priority received between 2024-07-05 and 2024-07-12.\",\n",
      "                \"start\": \"2024-07-05\",\n",
      "                \"end\": \"2024-07-12\"\n",
      "            }\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "retrieval = client.chat.completions.create(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    response_model=Retrieval,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are Jason's personal assistant.\n",
    "                He has two emails jason@work.com jason@personal.com\n",
    "                Today is {date.today()}\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What meetings do I have today and are there any important emails I should be aware of\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(retrieval.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4) Decomposing questions\n",
    "\n",
    "Lastly, a lightly more complex example of a problem that can be solved with structured output is decomposing questions. Where you ultimately want to decompose a question into a series of sub-questions that can be answered by a search backend. For example\n",
    "\n",
    "\"Whats the difference in populations of jason's home country and canada?\"\n",
    "\n",
    "You'd ultimately need to know a few things\n",
    "\n",
    "1. Jason's home country\n",
    "2. The population of Jason's home country\n",
    "3. The population of Canada\n",
    "4. The difference between the two\n",
    "\n",
    "This would not be done correctly as a single query, nor would it be done in parallel, however there are some opportunities try to be parallel since not all of the sub-questions are dependent on each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-3-sonnet-20240229-v1:0/invoke \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"root_question\": \"What is the difference between the population of jason's home country and canada?\",\n",
      "    \"plan\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"query\": \"What is jason's home country?\",\n",
      "            \"subquestions\": []\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"query\": \"What is the population of jason's home country?\",\n",
      "            \"subquestions\": [\n",
      "                1\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"query\": \"What is the population of canada?\",\n",
      "            \"subquestions\": []\n",
      "        },\n",
      "        {\n",
      "            \"id\": 4,\n",
      "            \"query\": \"Calculate the difference between the population of jason's home country and the population of canada\",\n",
      "            \"subquestions\": [\n",
      "                2,\n",
      "                3\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Question(BaseModel):\n",
    "    id: int = Field(..., description=\"A unique identifier for the question\")\n",
    "    query: str = Field(..., description=\"The question decomposited as much as possible\")\n",
    "    subquestions: List[int] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"The subquestions that this question is composed of\",\n",
    "    )\n",
    "\n",
    "\n",
    "class QueryPlan(BaseModel):\n",
    "    root_question: str = Field(..., description=\"The root question that the user asked\")\n",
    "    plan: List[Question] = Field(\n",
    "        ..., description=\"The plan to answer the root question and its subquestions\"\n",
    "    )\n",
    "\n",
    "\n",
    "retrieval = client.chat.completions.create(\n",
    "    model=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    response_model=QueryPlan,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a query understanding system capable of decomposing a question into subquestions.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the difference between the population of jason's home country and canada?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(retrieval.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope in this section I've exposed you to some ways we can be creative in modeling structured outputs to leverage LLMS in building some lightweight components for our systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
